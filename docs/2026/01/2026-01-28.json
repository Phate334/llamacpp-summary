{"executed_at":"2026-01-29T01:52:24Z","data":[{"tag":"b7865","published_at":"2026-01-28T21:49:39Z","body":"Vulkan Flash Attention Coopmat1 Refactor (#19075)\n\n* vulkan: use coopmat for flash attention p*v matrix multiplication\n\n* fix P loading issue\n\n* fix barrier position\n\n* remove reduction that is no longer needed\n\n* move max thread reduction into loop\n\n* remove osh padding\n\n* add bounds checks and padding\n\n* remove unused code\n\n* fix shmem sizes, loop duration and accesses\n\n* don't overwrite Qf, add new shared psh buffer instead\n\n* add missing bounds checks\n\n* use subgroup reductions\n\n* optimize\n\n* move bounds check, reduce barriers\n\n* support other Bc values and other subgroup sizes\n\n* remove D_split\n\n* replace Of register array with shared memory Ofsh array\n\n* parallelize HSV across the rowgroups\n\n* go back to Of in registers, not shmem\n\n* vectorize sfsh\n\n* don't store entire K tile in shmem\n\n* fixes\n\n* load large k tiles to shmem on Nvidia\n\n* adapt shared memory host check function to shader changes\n\n* remove Bc 32 case\n\n* remove unused variable\n\n* fix missing mask reduction tmspsh barrier\n\n* fix mask bounds check\n\n* fix rowmax f16 under/overflow to inf\n\n* fix flash_attn_cm2 BLOCK_SIZE preprocessor directives"},{"tag":"b7864","published_at":"2026-01-28T21:43:06Z","body":"spec : add self‑speculative decoding (no draft model required) + refactor (#18471)\n\n* server: introduce self-speculative decoding\n\n* server: moved self-call into speculative.cpp\n\n* can_speculate() includes self-speculation\n\nCo-authored-by: Georgi Gerganov <ggerganov@gmail.com>\n\n* server: can_speculate() tests self-spec\n\n* server: replace can_speculate() with slot.can_speculate()\n\nCo-authored-by: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>\n\n* common: use %zu format specifier for size_t in logging\n\nCo-authored-by: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>\n\n* server: can_speculate() requires a task instance\n\n* common: ngram map, config self-speculative decoding\n\n* common: add enum common_speculative_type\n\n* common: add vector of speculative states\n\n* common: add option --spec-draftless\n\n* server: cleanup (remove slot.batch_spec, rename)\n\n* common: moved self-spec impl to ngram-map\n\n* common: cleanup (use common_speculative_state_draft)\n\n* spec : refactor\n\n* cont : naming\n\n* spec: remove --spec-config\n\n* doc: (draftless) speculative decoding\n\n* common: print performance in spec decoding\n\n* minor : cleanup\n\n* common : better names\n\n* minor : cleanup + fix build\n\n* minor: comments\n\n* CODEOWNERS: add common/ngram-map.* (#18471)\n\n* common : rename speculative.draftless_type -> speculative.type\n\n* ngram-map : fix uninitialized values\n\n* ngram-map : take into account the input can become shorter\n\n* ngram-map : revert len check for now\n\n* arg : change `--spec-draftless` -> `--spec-type`\n\n* spec : add common_speculative_state::accept()\n\n* spec : refactor + add common_speculative_begin()\n\n* spec : fix begin() call with mtmd\n\n* spec : additional refactor + remove common_speculative_params\n\n---------\n\nCo-authored-by: Georgi Gerganov <ggerganov@gmail.com>\nCo-authored-by: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>"},{"tag":"b7862","published_at":"2026-01-28T21:29:16Z","body":"ggml-sycl: remove unused syclcompat header (#19140)\n\nThe syclcompat/math.hpp is not used anymore. The change that intrduced it was successfuly reverted (https://github.com/ggml-org/llama.cpp/pull/17826).\nThis include path will become obsolete and dropped in oneAPI 2026.0 effectively breaking ggml-sycl builds."},{"tag":"b7861","published_at":"2026-01-28T21:05:53Z","body":"jinja : undefined should be treated as sequence/iterable (return string/array) by filters/tests (#19147)\n\n* undefined is treated as iterable (string/array) by filters\n\n`tojson` is not a supported `undefined` filter\n\n* add tests\n\n* add sequence and iterable tests\n\nkeep it DRY and fix some types"},{"tag":"b7860","published_at":"2026-01-28T19:33:37Z","body":"vulkan: handle device dedup on MacOS + Vega II Duo cards (#19058)\n\nDeduplication here relied on the fact that vulkan would return unique\nUUID for different physical GPUs. It is at the moment not always the case.\nOn Mac Pro 2019 running Mac OS, with 2 Vega II Duo cards (so, 4 GPU total),\nMotlenVK would assign same UUID to pairs of GPUs, unless they\nare connected with Infinity Fabric.\n\nSee more details here: KhronosGroup/MoltenVK#2683.\n\nThe right way is to fix that in MoltenVK, but until it is fixed,\nllama.cpp would only recognize 2 of 4 GPUs in such configuration.\n\nThe deduplication logic here is changed to only filter GPUs if UUID is\nsame but driver is different."},{"tag":"b7858","published_at":"2026-01-28T17:49:27Z","body":"ggml: new backend for Virglrenderer API Remoting acceleration (v2) (#18718)"},{"tag":"b7857","published_at":"2026-01-28T14:43:53Z","body":"ggml-cpu: arm64: Q4_K scale unroll and vectorization (#19108)"},{"tag":"b7856","published_at":"2026-01-28T14:08:13Z","body":"cuda : fix \"V is K view\" check for non-unified KV cache (#19145)"},{"tag":"b7855","published_at":"2026-01-28T11:49:30Z","body":"CUDA: tune GLM 4.7 Flash FA kernel selection logic (DGX Spark) (#19142)"},{"tag":"b7853","published_at":"2026-01-28T10:53:34Z","body":"llama : disable Direct IO by default (#19109)\n\n* llama : disable Direct IO by default\n\n* cont : override mmap if supported"},{"tag":"b7852","published_at":"2026-01-28T06:19:46Z","body":"sampling : remove sampling branching in output_reserve (#18811)\n\n* sampling : remove sampling branching in output_reserve\n\nThis commit updates output_reserve in llama-context.cpp to always\nallocate sampling buffers regardless of whether sampling is needed for\nthe current batch.\n\nThe motivation for this is to avoid reallocations and branching based on\nthe sampling requirements of the batch."},{"tag":"b7851","published_at":"2026-01-28T06:08:04Z","body":"ggml webgpu: Split shared state (webgpu_context) into global state and per-thread state (#18976)\n\n* Squashed commit of the following:\n\ncommit b3c6bf4b0450d8d452b934df27a0fb7cb53cd755\nAuthor: Abhijit Ramesh <abhijitramesh2k@gmail.com>\nDate:   Mon Dec 1 18:29:00 2025 -0800\n\n    ggml webgpu: fix xielu parameter passing (#11)\n\n    The XIELU operation was incorrectly using static_cast to convert\n    float parameters to uint32_t, which converted numeric values instead\n    of preserving IEEE 754 bit patterns. This caused incorrect values\n    to be interpreted by the GPU shader.\n\n    * Use reinterpret_cast to preserve float bit patterns when passing\n      through uint32_t params buffer\n    * Update WGSL shader parameter types from u32 to f32\n    * Re-enable XIELU support (was disabled due to numerical issues)\n\n    Fixes NMSE test failures for XIELU operation on WebGPU backend.\n\ncommit 5ca9b5e49ea7cddc9ab7c8b43a11a9c76a4dff4a\nAuthor: neha-ha <137219201+neha-ha@users.noreply.github.com>\nDate:   Tue Nov 18 12:17:00 2025 -0800\n\n    Refactored pipelines and workgroup calculations (#10)\n\n    * refactored pipelines\n\n    * refactored workgroup calculation\n\n    * removed commented out block of prior maps\n\n    * Clean up ceiling division pattern\n\n    ---------\n\n    Co-authored-by: Neha Abbas <nehaabbas@eduroam-169-233-141-223.ucsc.edu>\n    Co-authored-by: Reese Levine <reeselevine1@gmail.com>\n\nAuthor: James Contini <jamescontini@gmail.com>\nDate:   Wed Oct 29 23:13:06 2025 -0700\n\n    formatted embed wgsl and ggml-webgpu.cpp\n\ncommit e1f6baea31645e5d96ad53664acae856f74b96f4\nAuthor: James Contini <jamescontini@gmail.com>\nDate:   Wed Oct 29 23:08:37 2025 -0700\n\n    implemented REPL_Template support and removed bug in unary operators kernel\n\ncommit 8c70b8fece445cdc9a8c660dbddbf201e52da2bb\nAuthor: James Contini <jamescontini@gmail.com>\nDate:   Wed Oct 15 16:14:20 2025 -0700\n\n    responded and dealt with PR comments\n\ncommit f9282c660c10dec4487d434549bdb707a9cd9f37\nAuthor: James Contini <jamescontini@gmail.com>\nDate:   Sun Oct 12 13:41:41 2025 -0700\n\n    removed unnecesarry checking if node->src[1] exists for unary operators\n\ncommit 4cf28d7dec41c29186d66152735b244c5699f9dc\nAuthor: James Contini <jamescontini@gmail.com>\nDate:   Sun Oct 12 13:32:45 2025 -0700\n\n    All operators (inlcluding xielu) working\n\ncommit 74c6add1761a59d2c2ff60b60e8ad3c8300f6d3e\nAuthor: James Contini <jamescontini@gmail.com>\nDate:   Fri Oct 10 13:16:48 2025 -0700\n\n    fixed autoconfig\n\ncommit 362749910be4f0120c8ffb21ceddeb7d2c088e51\nAuthor: James Contini <jamescontini@gmail.com>\nDate:   Fri Oct 10 13:10:46 2025 -0700\n\n    removed vestigial files\n\ncommit cb0858333785757804c5104e59c4981843207c16\nAuthor: James Contini <jamescontini@gmail.com>\nDate:   Fri Oct 10 12:59:32 2025 -0700\n\n    abides by editor-config\n\ncommit 5360e2852a4b51197d7d67d0a5d42e908b02d7ed\nAuthor: James Contini <jamescontini@gmail.com>\nDate:   Fri Oct 10 12:45:57 2025 -0700\n\n    rms_norm double declaration bug atoned\n\ncommit 7b09baa4aa53711be5a126043670cc182c78bfcd\nMerge: 8a6ec843 74b8fc17\nAuthor: James Contini <jamescontini@gmail.com>\nDate:   Fri Oct 10 11:50:03 2025 -0700\n\n    resolving merge conflicts\n\ncommit 8a6ec843a50ab82f8cef59b4558eb63f318ba02d\nAuthor: James Contini <jamescontini@gmail.com>\nDate:   Wed Oct 8 18:06:47 2025 -0700\n\n    unary operators pass ggml tests\n\ncommit c3ae38278a2db236adc5912c9140e4f0d63f2c19\nAuthor: James Contini <jamescontini@gmail.com>\nDate:   Wed Oct 1 16:22:40 2025 -0700\n\n    neg passes backend test\n\ncommit aa1c9b2f8877a405470ca56709c42a1fd43713de\nAuthor: James Contini <jamescontini@gmail.com>\nDate:   Tue Sep 30 23:55:27 2025 -0700\n\n    neg f16xf32xip builds and runs, havent actually ran a model that uses neg kernel yet though\n\nCo-authored-by: James Contini <jamescontini@gmail.com>\nCo-authored-by: Neha Abbas <neabbas@ucsc.edu>\nCo-authored-by: Abhijit Ramesh <abhijitramesh2k@gmail.com>\n\n* Remove extra code and format\n\n* Add ops documentation (finally)\n\n* ggml webgpu: add SOFTPLUS unary operator\n\nImplements SOFTPLUS (log(1 + exp(x))) with f16/f32 support. Uses f32\nprecision for intermediate calculations to prevent f16 overflow.\n\n* Add shader implementation and 4 variants (f32/f16, inplace/non-inplace)\n* Register pipelines and device support\n* Follow Vulkan backend numerical stability pattern\n\n* ggml webgpu: add EXPM1 unary operator\n\nImplements EXPM1 (exp(x) - 1) with f16/f32 support.\n\n* Add shader implementation and 4 variants (f32/f16, inplace/non-inplace)\n* Register pipelines and device support\n\n* ggml webgpu: add FLOOR unary operator\n\nImplements FLOOR (rounds down to nearest integer) with f16/f32 support.\n\n* Add shader implementation and 4 variants (f32/f16, inplace/non-inplace)\n* Register pipelines and device support\n\n* ggml webgpu: add CEIL unary operator\n\nImplements CEIL (rounds up to nearest integer) with f16/f32 support.\n\n* Add shader implementation and 4 variants (f32/f16, inplace/non-inplace)\n* Register pipelines and device support\n\n* ggml webgpu: add ROUND unary operator\n\nImplements ROUND (rounds to nearest integer) with f16/f32 support.\n\n* Add shader implementation and 4 variants (f32/f16, inplace/non-inplace)\n* Register pipelines and device support\n\n* ggml webgpu: add TRUNC unary operator\n\nImplements TRUNC (truncates towards zero) with f16/f32 support.\n\n* Add shader implementation and 4 variants (f32/f16, inplace/non-inplace)\n* Register pipelines and device support\n\n* docs : update WebGPU support for unary operators (FLOOR, CEIL, ROUND, TRUNC, EXPM1, SOFTPLUS)\n\n* Updates to webgpu get_memory\n\n* Move shared state (webgpu_context) and device creation out of registration context, device context, and buffer context, and move into backend context\n\n* Small cleanup\n\n* Move Instance, Device, Adapter, Device creation, and capabilities to global state while moving Queue, pipelines, and buffers to per-thread state.\n\n* Cleanups\n\n* More cleanup\n\n* Move staging_buf mutex to global context\n\n* Resolve merge\n\n* Resolve merge\n\n* Resolve merge\n\n* Clean up merge errors, delete forward declaration, and run clang-format\n\n* Rename device_init to backend_init\n\n* Move webgpu_context to backend_context\n\n* Move buffer context members into global context and refactor function calls\n\n* Run clang-format\n\n* Remove commends\n\n* Move parameter buffers to per-thread, add single memset_tensor param buf\n\n* Fix CI compilation issue\n\n* Fix builds for emscripten not supporting subgroups\n\n* cleanup\n\n* cleanup\n\n---------\n\nCo-authored-by: Reese Levine <reeselevine1@gmail.com>"}]}
