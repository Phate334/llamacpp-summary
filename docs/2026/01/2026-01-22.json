{"executed_at":"2026-01-23T01:51:38Z","data":[{"tag":"b7812","published_at":"2026-01-22T22:32:57Z","body":"mla : make the V tensor a view of K (#18986)\n\n* mla : pass V as a view of K to the FA op\n\n* cuda : adjust mla logic to new layout\n\n* kv-cache : fix rope shift\n\n* tests : remove comment\n\n* cuda : fix reusable_cutoff\n\nCo-authored-by: Johannes Gäßler <johannesg@5d6.de>\n\n---------\n\nCo-authored-by: Johannes Gäßler <johannesg@5d6.de>"},{"tag":"b7811","published_at":"2026-01-22T22:23:24Z","body":"CUDA: fix alignment check for FA (#19023)"},{"tag":"b7809","published_at":"2026-01-22T20:55:07Z","body":"opencl: enable the general fp mm for non-cont input and as a fallback for specialized kqv kernel for adreno (#18970)\n\n* opencl: add `copy_to_contiguous` and utilize mm kernels\n\n* opencl: only copy to cont for f32 and f16 tensors\n\n* opencl: use cont mm for fallback when dst is large\n\n* opencl: use nb local to copy-to-cont\n\n* opencl: use local offset as well"},{"tag":"b7808","published_at":"2026-01-22T19:53:30Z","body":"server: do not log certain endpoints (avoid log spam) (#19028)"},{"tag":"b7807","published_at":"2026-01-22T17:43:21Z","body":"quant : manual overrides of tensor types take precedence (#18952)"},{"tag":"b7806","published_at":"2026-01-22T17:48:48Z","body":"release: update github api (#19022)"},{"tag":"b7805","published_at":"2026-01-22T17:34:49Z","body":"mtmd : update docs to use llama_model_n_embd_inp (#18999)"},{"tag":"b7804","published_at":"2026-01-22T17:30:25Z","body":"server: Reorder methods in `server-task.cpp` (#19016)\n\n* Move `task_result_state::update_chat_msg` to match with header\n\n* Move `server_task_result_cmpl_partial::to_json_anthropic()` to match with header\n\n---------\n\nCo-authored-by: openingnow <>"},{"tag":"b7802","published_at":"2026-01-22T06:50:34Z","body":"opencl: add TRI op support (#18979)"},{"tag":"b7801","published_at":"2026-01-22T02:25:39Z","body":"ggml-zdnn : mark zDNN buffers as non-host (#18967)\n\nWhile buffers reside in host memory,\nadditional transformation is needed to use buffers with zDNN.\n\nFixes #18848"},{"tag":"b7798","published_at":"2026-01-22T01:40:40Z","body":"jinja: support none|string (#18995)\n\n* jinja: support none|string\n\n* Update common/jinja/value.cpp\n\nCo-authored-by: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>\n\n* Update tests/test-jinja.cpp\n\nCo-authored-by: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>\n\n* Add as_string()\n\n---------\n\nCo-authored-by: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>"},{"tag":"b7795","published_at":"2026-01-22T01:07:27Z","body":"vulkan: Remove transfer_ctx, do everything in compute_ctx. (#18945)\n\n* vulkan: Remove transfer_ctx, do everything in compute_ctx.\n\nWe had a bug where a set_tensor_async (using transfer_ctx) didn't get\nsubmitted before the graph_compute (using compute_ctx) that came after\nit. To avoid this sort of issue, just do everything in compute_ctx.\n\nRemove transfer_cmd_pool, which was already unused.\n\n* fix crash with perf logger"},{"tag":"b7794","published_at":"2026-01-22T00:58:54Z","body":"common : improve error message when HTTPS is missing but required (#18987)\n\nSigned-off-by: Adrien Gallouët <angt@huggingface.co>"},{"tag":"b7793","published_at":"2026-01-22T00:28:27Z","body":"server: /v1/responses (partial) (#18486)\n\n* from previous PR\n\n* Make instruction(system) as first message\n\n* Convert [input_message] (text/image/file)\n\n* Rename convert_responses_to_chatcmpl(body) -> response_body\n\n* Initial tool call support\n\n* Erase instructions field from chatcmpl body\n\n* Feed reasoning texts to chat template\n\n* Use std::vector instead of opaque json array\n\n* Make output_item.added events consistent\n\n* Move `server_task_result_cmpl_partial::update` from header to source\n\n* Match ID of output_item.added and .done events\n\n* Add function_call only if there is no \"fc_\" prefix\n\n* Add function call output at non-streaming API\n\n* Test if ID is persistent\n\n* Add doc\n\n* Fix style - use trailing comma\n\n* Rewrite state management\n\n* catch up with upstream/master\n\n* Fix style - \"type\" is the first item of SSE data\n\n* Explicitly check \"instructions\" from response_body\n\n* Make lambdas static\n\n* Check if reasoning content exists\n\n* Add `oai_resp_id` to task_result_state(also initialized at ctor), server_task_result_cmpl_partial, and server_task_result_cmpl_final\n\n* Reject `input_file` since it is not supported by chatcmpl\n\n* Add \"fc_\" prefix to non-straming function call id as coderabbit pointed out\n\n---------\n\nCo-authored-by: openingnow <>"},{"tag":"b7792","published_at":"2026-01-22T00:20:14Z","body":"vulkan: support flash attention GQA/split_k with small batches (#18938)"},{"tag":"b7791","published_at":"2026-01-22T00:15:21Z","body":"Revert \"vulkan: force full subgroups for flash attention to fix intel subgroup crash (#17356)\" (#18831)\n\nThis reverts commit 980b7cd17e055c8c587f79ffda7eb4fddf405566."}]}
