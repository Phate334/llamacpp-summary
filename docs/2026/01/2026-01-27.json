{"executed_at":"2026-01-28T01:51:48Z","data":[{"tag":"b7849","published_at":"2026-01-27T19:58:26Z","body":"jinja : implement mixed type object keys (#18955)\n\n* implement mixed type object keys\n\n* add tests\n\n* refactor\n\n* minor fixes\n\n* massive refactor\n\n* add more tests\n\n* forgotten tuples\n\n* fix array/object is_hashable\n\n* correct (albeit broken) jinja responses\n\nverified with transformers\n\n* improved hashing and equality\n\n* refactor hash function\n\n* more exhausive test case\n\n* clean up\n\n* cont\n\n* cont (2)\n\n* missing cstring\n\n---------\n\nCo-authored-by: Xuan Son Nguyen <son@huggingface.co>"},{"tag":"b7847","published_at":"2026-01-27T16:32:03Z","body":"CUDA: tune GLM 4.7 Flash FA kernel selection logic (#19097)"},{"tag":"b7845","published_at":"2026-01-27T11:06:29Z","body":"ggml-cpu: aarm64: q6_K repack gemm and gemv (and generic) implementations (i8mm) #18860 (#18888)\n\n* Boilerplate for q6_K repack\n\n* q6_K repack to q6_Kx8 implementation\n\nSigned-off-by: Alberto Cabrera <alberto.cabrera@liquid.ai>\n\n* q6_K generic gemv and gemm\n\n* wip, gemm_q6_K 8x8\n\n* Still WIP: loading of q8s, q6h and q6l\n\n* first working version of q6_K gemm\n\n* Moved q6 loads outside of sb block, Unrolled inner loop\n\n* Replaced modulo with mask\n\n* First implementation of GEMV\n\n* ggml_vdotq_s32 -> vdotq_s32\n\n* Reduce width of accumulators in q6_K gemv\n\n* Bsums instead of calc bias. Preload scales to use vget_lane. Unroll.\n\n* Reuse scales in GEMM (same GEMV opt)\n\n* Added todos for bsum and different qh repack\n\n* Arch fallback\n\n* VSLIQ for merging qh adn ql\n\n* Removed TODO, already tested\n\n* Apply suggestions\n\nCo-authored-by: Georgi Gerganov <ggerganov@gmail.com>\n\n* Removed unused import\n\n---------\n\nSigned-off-by: Alberto Cabrera <alberto.cabrera@liquid.ai>\nCo-authored-by: Georgi Gerganov <ggerganov@gmail.com>"},{"tag":"b7844","published_at":"2026-01-27T09:08:29Z","body":"[CUDA] Reduce CPU-side stalls due to the CUDA command buffer being full (#19042)\n\n* [CUDA] Reduce CPU-side stalls due to the CUDA command buffer being full\n\nWith pipeline parallelism, during prompt processing, the CPU-side CUDA command buffer gets full, stalling the CPU. Due to this, enough work doesn't get submitted to the GPU, causing bubbles in the GPU timeline.\nFix this by setting the CUDA environment variable CUDA_SCALE_LAUNCH_QUEUES to 4x to increase the command buffer size.\n\n* Set the env variable in the CUDA backend registry allocation\n\n* Add link to PR in code comment\n\n* Remove warning logs and update documentation"},{"tag":"b7843","published_at":"2026-01-27T05:57:34Z","body":"common : clarify HTTPS build options in error message (#19103)\n\n* common : clarify HTTPS build options in error message\n\nThis commit updates the https error message to provide clearer\ninstructions for users who encounter the \"HTTPS is not supported\" error.\n\nThe motivation for this is that it might not be clear to users that only\none of these options are needed to enable HTTPS support.\nThe LLAMA_OPENSSL option is also added to the message to cover all\npossible build configurations.\n\n* clarify that OpenSSL is the default for HTTPS support"},{"tag":"b7842","published_at":"2026-01-27T05:14:45Z","body":"ggml-cpu: Enable FP16 MMA kernels on PPC (#19060)"},{"tag":"b7841","published_at":"2026-01-27T04:33:33Z","body":"opencl: add flattened q6_K mv (#19054)\n\n* opencl: flatten `q6_K` and add `kernel_mul_mv_q6_K_f32_flat`\n\n* opencl: clean up\n\n* opencl: refactor q6_K mv - put loop body in `block_q_6_K_dot_y_flat`\n\n* opencl: tweak the workgroup size a bit\n\n* opencl: output 4 values per subgroup for `kernel_mul_mv_q6_K_f32_flat`\n\n* opencl: proper alignment for q6_K\n\n* opencl: boundary handling for flattened q6_K mv\n\n* opencl: rename q6_K mv kernel file\n\n* opencl: put flattened q6_K mv in its own file\n\n* opencl: use lower k in file name\n\n* opencl: use K in variable names"}]}
