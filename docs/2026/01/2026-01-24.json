{"executed_at":"2026-01-25T01:52:08Z","data":[{"tag":"b7825","published_at":"2026-01-24T22:03:58Z","body":"llama-fit-params: keep explicit --ctx-size 0 (#19070)"},{"tag":"b7824","published_at":"2026-01-24T21:39:33Z","body":"GGUF: check that tensor size is representable (#19072)"},{"tag":"b7823","published_at":"2026-01-24T17:52:19Z","body":"chat: fix language input for translategemma (#19052)\n\n* chat: fix language input for translategemma\n\n* Update common/chat.cpp\n\nCo-authored-by: Aldehir Rojas <hello@alde.dev>\n\n---------\n\nCo-authored-by: Aldehir Rojas <hello@alde.dev>"},{"tag":"b7822","published_at":"2026-01-24T10:07:25Z","body":"CUDA: re-use MLA K data for V in MMA FA (#19057)"},{"tag":"b7821","published_at":"2026-01-24T08:25:47Z","body":"ggml-cuda: enable cuda-graphs for `n-cpu-moe` (#18934)\n\n* ggml-cuda: add split-wise cuda graph\n\n* add n-cpu-moe compare_llama_bench.py\n\n* fix hip/musa builds"},{"tag":"b7820","published_at":"2026-01-24T07:26:57Z","body":"ggml-hexagon: flash-attn opt (#19025)\n\n* optimize flash attention kernel by improving score computation and online softmax update\n\n* wip\n\n* Refactor online softmax update in flash attention kernel for improved performance\n\n* Optimize flash attention kernel by replacing float array with HVX_Vector for score computation\n\n* wip"}]}
