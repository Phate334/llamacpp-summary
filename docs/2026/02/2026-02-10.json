{"executed_at":"2026-02-11T01:53:04Z","data":[{"tag":"b7991","published_at":"2026-02-10T21:24:39Z","body":"[WebGPU] Plug memory leaks and free resources on shutdown (#19315)\n\n* Fix memory leaks in shader lib, backend, backend_context, buffer_context, and webgpu_buf_pool\n\n* Free pools\n\n* Cleanup\n\n* More cleanup\n\n* Run clang-format\n\n* Fix arg-parser and tokenizer test errors that free an unallocated buffer\n\n* Fix device lost callback to not print on device teardown\n\n* Fix include and run clang-format\n\n* remove unused unused\n\n* Update binary ops\n\n---------\n\nCo-authored-by: Reese Levine <reeselevine1@gmail.com>"},{"tag":"b7990","published_at":"2026-02-10T21:22:13Z","body":"models : support qwen3.5 series (#19468)\n\n* support qwen3.5 series\n\n* remove deepstack for now, and some code clean\n\n* code clean\n\n* add FULL_ATTENTION_INTERVAL metadata\n\n* code clean\n\n* reorder v heads for linear attention to avoid expensive interleaved repeat"},{"tag":"b7989","published_at":"2026-02-10T21:16:45Z","body":"test: fix IMROPE perf test case (#19465)"},{"tag":"b7988","published_at":"2026-02-10T17:52:34Z","body":"ggml-cpu: arm64: q6_K repack gemm and gemv (and generic) implementations (dotprod) (#19360)\n\n* First working version of GEMM and GEMV\n\n* interleave loads and compute\n\n* Clang-format\n\n* Added missing fallback. Removed tested TODO.\n\n* Swap M and N to be consistent with the repack template convention"},{"tag":"b7987","published_at":"2026-02-10T16:21:39Z","body":"ggml : use noexcept overload for is_regular_file in backend registration (#19452)\n\nusing noexcept std::filesystem::directory_entry::is_regular_file\noverload prevents abnormal termination upon throwing an error\n(as caused by symlinks to non-existent folders on linux)\n\nResolves: #18560"},{"tag":"b7984","published_at":"2026-02-10T11:02:44Z","body":"CANN: Remove unnecessary wrapper for `gml_backend_buft_is_cann` (#18968)"},{"tag":"b7983","published_at":"2026-02-10T10:35:33Z","body":"CANN: implement quantized MUL_MAT_ID for MoE models (#19228)\n\nImplement ggml_cann_mul_mat_id_quant function to support quantized matrix\nmultiplication for Mixture of Experts (MoE) architectures on CANN backend.\n\nKey features:\n- Support Q4_0 and Q8_0 quantized weight formats\n- Use IndexSelect to dynamically route expert-specific weights based on indices\n- Leverage WeightQuantBatchMatmulV2 for efficient quantized computation\n- Handle automatic F16 type conversion for hardware compatibility\n- Support both per-expert and broadcast input modes\n\nImplementation details:\n- Extract expert weights and scales using CANN IndexSelect operation\n- Process each batch and expert combination independently\n- Create proper tensor views with correct stride for matmul operations\n- Automatic input/output type casting to/from F16 as needed\n\nTesting: All test cases passed for supported types (F32, F16, Q4_0, Q8_0)."},{"tag":"b7982","published_at":"2026-02-10T09:22:40Z","body":"cuda : extend GGML_OP_PAD to work with non-cont src0 (#19429)\n\n* cuda : extend GGML_OP_PAD to work with non-cont src0\n\n* tests : add permuted pad"},{"tag":"b7981","published_at":"2026-02-10T04:46:19Z","body":"chat: fix case where template accepts type content only (#19419)\n\n* chat: fix case where template accepts type content only\n\n* rm stray log\n\n* reuse render_message_to_json"}]}
