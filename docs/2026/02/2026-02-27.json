{"executed_at":"2026-02-28T01:51:54Z","data":[{"tag":"b8179","published_at":"2026-02-27T20:28:00Z","body":"CUDA: add CDNA3 MFMA support for flash attention MMA kernel (#19806)\n\n* CUDA: add CDNA3 MFMA support for flash attention MMA kernel\n\nAdd MI300X (gfx942) MFMA tensor core flash attention using\nv_mfma_f32_16x16x16_f16 (FP16 in, FP32 accumulate).\n\n- Add FATTN_WARP_SIZE=64 for CDNA wavefront64\n- Add CDNA config for head sizes 64, 80, 96, 112, 128\n- Add FP16 MFMA intrinsic path in mma.cuh\n- Add manual V transpose load for MFMA register layout\n- Route CDNA to MMA for prompt processing, VEC for token generation\n- Fix Q loading and combine stride granularity for non-power-of-2 heads\n\nBenchmarks (Qwen2.5-1.5B Q4_K_M, MI300X):\n  pp512  +7%,  pp1024 +13%,  pp2048 +23%,  pp4096 +39%\n  tg128  -10% (FA overhead, VEC used for both)\n\nAll 2480 flash attention tests pass.\n\nRef: https://github.com/ggml-org/llama.cpp/issues/17917\n\n* address review: replace FATTN_WARP_SIZE with constexpr, improve dispatch\n\n- Replace #define FATTN_WARP_SIZE with constexpr int warp_size =\n  ggml_cuda_get_physical_warp_size() in each device function\n- Use ne[1]*gqa_ratio threshold for MMA vs tile dispatch. Benchmarked\n  crossover on MI300X @ d32768 with power-of-2 GQA models:\n    hsk=64  (Llama 1B, gqa=4): MMA wins at eff >= 128 (+11%)\n    hsk=128 (Llama 3B, gqa=4): MMA wins at eff >= 128 (+4%)\n  Unified threshold: eff_nq >= 128 for all head sizes.\n- Remove VEC fallback; small batches fall through to tile kernel\n\n* Update ggml/src/ggml-cuda/fattn.cu\n\n* use ggml_cuda_info().devices warp_size instead of hardcoded check\n\n---------\n\nCo-authored-by: Johannes Gäßler <johannesg@5d6.de>"},{"tag":"b8178","published_at":"2026-02-27T19:42:28Z","body":"server: Add pragma once to server-context.h (#19944)"},{"tag":"b8177","published_at":"2026-02-27T18:15:10Z","body":"server: Mirroring /v1/responses to /responses to match /v1/chat/completions pattern (#19873)"},{"tag":"b8175","published_at":"2026-02-27T14:52:03Z","body":"ggml-cpu: add repack for mxfp4 (#19738)"},{"tag":"b8173","published_at":"2026-02-27T09:12:28Z","body":"server : support multiple model aliases via comma-separated --alias (#19926)\n\n* server : support multiple model aliases via comma-separated --alias\n\n* server : update --alias description and regenerate docs\n\n* server : multiple model aliases and tags\n\n- address review feedback from ngxson\n- --alias accepts comma-separated values (std::set, no duplicates)\n- --tags for informational metadata (not used for routing)\n- aliases resolve transparently in router via get_meta/has_model\n- /v1/models exposes aliases and tags fields\n\n* regenerate docs\n\n* nits\n\n* server : use first alias as model_name for backward compat\n\naddress review feedback from ngxson\n\n* server : add single-model test for aliases and tags"},{"tag":"b8172","published_at":"2026-02-27T08:17:17Z","body":"tests :  enable test-chat out of tree build (#19558)\n\nThe binary relies on model files that it tries to find. However, when\nconfiguring the build directory to be parallel to the source tree those\nheuristics fail.\n\nThis sets the working directory for the test executable to be the\nsource-tree which resolves this issue."},{"tag":"b8171","published_at":"2026-02-27T07:04:45Z","body":"replace the magic nunber 768 by max work group size to support iGPU (#19920)\n\nCo-authored-by: Neo Zhang Jianyu <jianyu.zhang@intel.com>"},{"tag":"b8170","published_at":"2026-02-27T05:40:43Z","body":"ggml-zendnn: update code for latest ZenDNN API (#19923)\n\n- adapt ggml-zendnn.cpp to the new lowoha::matmul interface\n- update the ZenDNN git tag in CMake to the latest release (ZenDNN‑2026‑WW08)\n- add static lib support in CMake"},{"tag":"b8169","published_at":"2026-02-27T03:25:24Z","body":"ggml : fix AMX and add batched support (#19925)\n\nllama-perplexity -hf ggml-org/Qwen3-0.6B-GGUF:Q4_0 -f wikitext-2-raw/wiki.test.raw -c 2048 -b 2048 --chunks 2\n\nbefore this commit:\n\n```\nperplexity: calculating perplexity over 2 chunks, n_ctx=2048, batch_size=2048, n_seq=1\nperplexity: 2.31 seconds per pass - ETA 0.07 minutes\n[1]17.3868,[2]22.2199,\nFinal estimate: PPL = 22.2199 +/- 1.59692\n\nllama_perf_context_print:        load time =     878.56 ms\nllama_perf_context_print: prompt eval time =    2037.82 ms /  4096 tokens (    0.50 ms per token,  2009.99 tokens per second)\nllama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:       total time =    6403.17 ms /  4097 tokens\nllama_perf_context_print:    graphs reused =          0\nllama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute    unaccounted |\nllama_memory_breakdown_print: |   - Host               |                  845 =   318 +     224 +     302                |\nllama_memory_breakdown_print: |   - CPU_REPACK         |                  288 =   288 +       0 +       0                |\nllama_memory_breakdown_print: |   - AMX                |                   31 =    31 +       0 +       0                |\n```\n\nafter this commit:\n\n```\nperplexity: calculating perplexity over 2 chunks, n_ctx=2048, batch_size=2048, n_seq=1\nperplexity: 1.98 seconds per pass - ETA 0.05 minutes\n[1]17.2005,[2]21.8220,\nFinal estimate: PPL = 21.8220 +/- 1.56485\n\nllama_perf_context_print:        load time =     719.23 ms\nllama_perf_context_print: prompt eval time =    1676.23 ms /  4096 tokens (    0.41 ms per token,  2443.58 tokens per second)\nllama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:       total time =    4258.74 ms /  4097 tokens\nllama_perf_context_print:    graphs reused =          0\nllama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute    unaccounted |\nllama_memory_breakdown_print: |   - Host               |                  845 =   318 +     224 +     302                |\nllama_memory_breakdown_print: |   - AMX                |                  319 =   319 +       0 +       0                |\n```\n(no more CPU_REPACK)\n\nafter this commit, disabling amx:\n\n```\nperplexity: calculating perplexity over 2 chunks, n_ctx=2048, batch_size=2048, n_seq=1\nperplexity: 2.34 seconds per pass - ETA 0.07 minutes\n[1]17.2005,[2]21.8220,\nFinal estimate: PPL = 21.8220 +/- 1.56485\n\nllama_perf_context_print:        load time =     841.91 ms\nllama_perf_context_print: prompt eval time =    2057.28 ms /  4096 tokens (    0.50 ms per token,  1990.98 tokens per second)\nllama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:       total time =    6454.51 ms /  4097 tokens\nllama_perf_context_print:    graphs reused =          0\nllama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute    unaccounted |\nllama_memory_breakdown_print: |   - Host               |                  845 =   318 +     224 +     302                |\nllama_memory_breakdown_print: |   - CPU_REPACK         |                  319 =   319 +       0 +       0                |\n```\n=> same perplexity.\n\nSigned-off-by: Adrien Gallouët <angt@huggingface.co>"},{"tag":"b8168","published_at":"2026-02-27T02:33:30Z","body":"vulkan: fix fp16 Flash Attention on Windows AMD RDNA2 and below (#19921)"},{"tag":"b8167","published_at":"2026-02-27T02:04:34Z","body":"mtmd : fix padding of n_tokens (#19930)"},{"tag":"b8166","published_at":"2026-02-27T02:03:30Z","body":"server : fix ctx checkpoint restore logic (#19924)"},{"tag":"b8165","published_at":"2026-02-27T01:26:19Z","body":"kv-cache : fix can_shift() check to take into account M-RoPE (#19928)"},{"tag":"b8164","published_at":"2026-02-27T00:34:49Z","body":"llama: Add option to merge gate and exp weights (#19139)\n\n* llama: Add option to merge gate and exp weights\n\n* Update convert_hf_to_gguf.py\n\nCo-authored-by: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>\n\n* Update convert_hf_to_gguf.py\n\nCo-authored-by: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>\n\n* update constants.py\n\n* add gate_up for the all MoE models\n\n* convert: simplify merge tensor condition\n\n* update constants.py\n\n* reduce number of models, add create_tensor_gate_up helper\n\n---------\n\nCo-authored-by: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>"}]}
