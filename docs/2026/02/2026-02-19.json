{"executed_at":"2026-02-20T01:52:26Z","data":[{"tag":"b8102","published_at":"2026-02-19T18:44:16Z","body":"model : add tokenizer from LFM2.5-Audio-1.5B (#19687)\n\n* model : Add tokenizer from LFM2.5-Audio-1.5B\n\n[LFM2.5-Audio-1.5B](https://huggingface.co/LiquidAI/LFM2.5-Audio-1.5B) introduced lightweight audio tokenizer.\n\nTokenizer based on LFM2 architecture and acts as \"embedding\" model with\ndifferent input `n_embd` and output `n_embd_out`.\n\nTo be used in https://github.com/ggml-org/llama.cpp/pull/18641.\n\nTo convert use\n\n```shell\npython3 convert_hf_to_gguf.py /path/to/LFM2.5-Audio-1.5B/audio_detokenizer\n```\n\n* Update convert_hf_to_gguf.py\n\nCo-authored-by: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>\n\n* Formatting\n\n* Rework check for attention layers\n\n* Add LFM2 SWA model support\n\n* Address PR feedback\n\n* Set vocab to none\n\n* Move helper function definitions to cpp file\n\n---------\n\nCo-authored-by: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>"},{"tag":"b8101","published_at":"2026-02-19T16:24:06Z","body":"llama : use output_resolve_row() in get_logits_ith/get_embeddings_ith (#19663)\n\nThis commit updates get_logits_ith(), and get_embeddings_ith() to use\noutput_resolve_row() to resolve the batch index to output row index.\n\nThe motivation for this is to remove some code duplication between these\nfunctions."},{"tag":"b8100","published_at":"2026-02-19T14:04:28Z","body":"model : full modern bert support (#18330)\n\n* full modern bert support\n\n* added gelu op in rank pooling for modern bert\n\n* still working on stuff, added mean calculation before classifier head\n\n* Update convert_hf_to_gguf.py\n\nCo-authored-by: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>\n\n* first layer is dense, as per modern bert research paper\n\n* Update src/llama-graph.cpp\n\nCo-authored-by: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>\n\n* fixed set input for mean pooling to check if pooling type is ranking since modern bert does mean & rank\n\n* Update src/llama-graph.cpp\n\nCo-authored-by: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>\n\n* Update convert_hf_to_gguf.py\n\nCo-authored-by: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>\n\n---------\n\nCo-authored-by: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>"},{"tag":"b8099","published_at":"2026-02-19T11:11:32Z","body":"llamafile: powerpc: add FP16 MMA path for Q4/Q8 matmul (#19709)\n\nAvoid xvi8ger4pp signed→unsigned bias correction by dequantizing Q4/Q8\ninputs to FP16 and using FP16×FP16→FP32 MMA. This removes\npost-processing overhead and improves performance.\n\nPerformance Impact:\n1.5 ~ 2x improvement in PP_Speed for Q4 and Q8 Models,\nmeasured with llama-bench and llama-batched-bench.\nQ8 Model: granite-4.0-h-micro-Q8_0.gguf (from huggingface)\nQ4 Model: Meta-Llama3-8b Q4 model (generated with llama-quantize from\nf32 model)\n\nllama-bench Q8 Model Results:\n model                          \t       size \t     params \t backend    \t threads \t            test \tBase t/s\tPatch t/s\n granitehybrid 3B Q8_0          \t   3.16 GiB \t     3.19 B \t CPU        \t      10 \t             pp8 \t         64.48 ± 4.72 \t         73.99 ± 0.27\n granitehybrid 3B Q8_0          \t   3.16 GiB \t     3.19 B \t CPU        \t      10 \t            pp16 \t         80.11 ± 0.32 \t        112.53 ± 0.40\n granitehybrid 3B Q8_0          \t   3.16 GiB \t     3.19 B \t CPU        \t      10 \t            pp32 \t         89.10 ± 0.27 \t        152.95 ± 0.68\n granitehybrid 3B Q8_0          \t   3.16 GiB \t     3.19 B \t CPU        \t      10 \t            pp64 \t         93.65 ± 0.25 \t        187.83 ± 0.83\n granitehybrid 3B Q8_0          \t   3.16 GiB \t     3.19 B \t CPU        \t      10 \t           pp128 \t         99.93 ± 0.02 \t        201.32 ± 0.11\n granitehybrid 3B Q8_0          \t   3.16 GiB \t     3.19 B \t CPU        \t      10 \t           pp256 \t        102.32 ± 0.40 \t        208.32 ± 0.41\n granitehybrid 3B Q8_0          \t   3.16 GiB \t     3.19 B \t CPU        \t      10 \t           pp512 \t        103.42 ± 0.40 \t        209.98 ± 0.14\n granitehybrid 3B Q8_0          \t   3.16 GiB \t     3.19 B \t CPU        \t      10 \t           tg128 \t         20.35 ± 0.01 \t         19.57 ± 0.01\n\nllama-bench Q4 Model Results:\n model                          \t       size \t     params \t backend    \t threads \t            test \t              Base    t/s \t               Patch   t/s\n llama 8B Q4_0                  \t   4.33 GiB \t     8.03 B \t CPU        \t      10 \t             pp8 \t         34.77 ± 0.10 \t         41.23 ± 0.08\n llama 8B Q4_0                  \t   4.33 GiB \t     8.03 B \t CPU        \t      10 \t            pp16 \t         40.81 ± 0.04 \t         64.55 ± 0.15\n llama 8B Q4_0                  \t   4.33 GiB \t     8.03 B \t CPU        \t      10 \t            pp32 \t         44.65 ± 0.05 \t         90.84 ± 0.22\n llama 8B Q4_0                  \t   4.33 GiB \t     8.03 B \t CPU        \t      10 \t            pp64 \t         47.49 ± 0.03 \t        114.39 ± 0.11\n llama 8B Q4_0                  \t   4.33 GiB \t     8.03 B \t CPU        \t      10 \t           pp128 \t         49.29 ± 0.24 \t        120.13 ± 0.19\n llama 8B Q4_0                  \t   4.33 GiB \t     8.03 B \t CPU        \t      10 \t           pp256 \t         49.77 ± 0.23 \t        121.51 ± 0.11\n llama 8B Q4_0                  \t   4.33 GiB \t     8.03 B \t CPU        \t      10 \t           pp512 \t         49.89 ± 0.23 \t        117.52 ± 0.10\n llama 8B Q4_0                  \t   4.33 GiB \t     8.03 B \t CPU        \t      10 \t           tg128 \t         13.40 ± 0.01 \t         13.37 ± 0.00\n\nLlama perplexity Results:\n\nModel\t                    Base Final PPL Estimate\tPatch Final PPL Estimate\ngranite-4.0-h-micro-Q8_0    1.3862 +/- 0.04424\t        1.3868 +/- 0.04432\nMeta-Llama3-8b Q4\t    1.3801 +/- 0.04116\t        1.3803 +/- 0.04116\n\nSigned-off-by: Shalini.Salomi.Bodapati <Shalini.Salomi.Bodapati@ibm.com>"},{"tag":"b8098","published_at":"2026-02-19T08:41:34Z","body":"models : dedup qwen35 graphs (#19660)\n\n* models : dedup qwen35 graphs\n\n* cont : add missing sigmoid"},{"tag":"b8095","published_at":"2026-02-19T00:52:50Z","body":"ggml webgpu: Fix bug in dispatching large matrix-vector multiplication (#19535)\n\n* Fix bug in dispatching large matrix-vector multiplication"}]}
