{"executed_at":"2026-02-26T01:52:22Z","data":[{"tag":"b8153","published_at":"2026-02-25T22:50:38Z","body":"server : enable multi-modal prompt caching (#19877)"},{"tag":"b8152","published_at":"2026-02-25T20:56:11Z","body":"server : support multi-modal context checkpoints (#19849)\n\n* Modify llama-memory-hybrid-iswa.cpp\n\n* Modify llama-memory-recurrent.cpp\n\n* Modify server-common.cpp\n\n* Modify server-common.h\n\n* Modify server-context.cpp\n\n* Modify server-task.h\n\n* Added comment to llama-memory-hybrid-iswa.cpp\n\n* Remove comment from server-context.cpp\n\n* Stylistic fix server-context.cpp\n\n* Fix an issue when seqrm isn't called in server-context.cpp\n\n* cont : alternative impl\n\n* cont : cleanup\n\n* cont : n_tokens -> int64_t\n\n---------\n\nCo-authored-by: timkhronos <timkhronos@gmail.com>"},{"tag":"b8149","published_at":"2026-02-25T06:49:56Z","body":"gguf : fix ftell/fseek for Windows (#19870)"},{"tag":"b8148","published_at":"2026-02-25T02:43:17Z","body":"models : fix graph splits (#19866)"}]}
