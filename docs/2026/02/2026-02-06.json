{"executed_at":"2026-02-07T01:52:19Z","data":[{"tag":"b7961","published_at":"2026-02-06T22:48:49Z","body":"sycl: add F16 support for GGML_OP_CEIL (#19306)\n\n* Fix SYCL CEIL operator\n\n* sycl: implement GGML_OP_CEIL"},{"tag":"b7960","published_at":"2026-02-06T22:33:52Z","body":"tests: reduce number of FA test permutations (#19381)\n\nOnly test non-F16 for head size 64 and 72 (one a multiple of QK, one not)."},{"tag":"b7959","published_at":"2026-02-06T22:28:50Z","body":"common : add common_speculative_is_compat() (#19270)\n\n* llama : add llama_memory_can_rm_suffix()\n\n* Revert \"llama : add llama_memory_can_rm_suffix()\"\n\nThis reverts commit d30e59b62a15ef4266a6503e3f4eba770aec001b.\n\n* spec : check if the target context is compatible for spec decoding"},{"tag":"b7958","published_at":"2026-02-06T22:07:41Z","body":"unicode : MSVC regex fix (#19340)\n\n* Fix model loading regex error\n\n* Change comments\n\n* Use const_iterator and remove specializations\n\n---------\n\nCo-authored-by: Alde Rojas <hello@alde.dev>"},{"tag":"b7957","published_at":"2026-02-06T18:55:07Z","body":"Kimi-Linear support (backend agnostic + MLA KV cache) (#18755)\n\n* kimi linear model implementation\n\n* kimi linear convert_hf_to_gguf\n\n* kimi linear constants.py tensor_mapping.py\n\n* Kimi Linear ggml.h\n\n* kimi linear ggml-cpu\n\n* Kimi Linear ggml-cuda\n\n* Kimi Linear ggml.c\n\n* kimi linear src/llama\n\n* remove \"const int64_t n_seq_tokens = q->ne[2];\" to get rid of unused variable warning\n\n* remove type mismatch warning\n\n* read MoE params\n\n* removed some hard coded code\n\n* removed all hard code\n\n* use DeepseekV2 tokenizer\n\n* removed unnecessary internal methods called by the old set_vocab of KimiLinear\n\n* rewrite get_vocab for KimiLinear. Removed all kda_scan code\n\n* removed all traces of kda_scan\n\n* reduce OP count by 1 due to removal of kda_scan\n\n* Move KIMI_LINEAR to llm_arch_is_hybrid to enable KV cache\n\n* set n_embd_head_k/v to ensure kv cache works\n\n* don't quantize conv1d of Kimi Linear\n\n* Kimi Linear backend agnostic\n\n* removed LOG_INFO\n\n* naive chunking form implemented\n\n* fixed some comments\n\n* add Kimi-K2 specific tokens to be recognized as EOG\n\n* build_kda_autoregressive is implemented to replace build_kda_recurrent for faster inference. sync'd to b7682\n\n* replaced Akk and Aqk with mul_mat and clamp\n\n* no clamp version\n\n* Moved Aqk computation out of the loop\n\n* fixed typo and split wkv_b into wk_b and wv_b\n\n* MLA KV cache support\n\n* fix trailing spaces\n\n* moved const llama_model & model; around to follow qwen3next format and see if it cna pass the -Wunused-private-field error\n\n* fix trailing whitespace\n\n* removed traling whitespaces in empty line + make sure indentation is multiple of 4\n\n* try to make lint happy\n\n* remove blank lines to make lint happy\n\n* removed at least blank line containing white space\n\n* fixed flake8 complaints locally\n\n* return ggml_tensor * pair in kda_autoregressive and kda_chunking as in ngxson's Qwen3Next improvement\n\n* removed Kimi-Linear specific change that causes failure at server-windows\n\n* removed private: from kimi_linear to make build checks happy\n\n* removed unnecessary ggml_cont before ggml_reshape\n\n* created static function causal_conv1d to abtract similar code for q/k/v\n\n* merged dt_bias to SSM_DT. Do -exp(log_A) in convert_hf_to_gguf.py.\n\n* reverted to original\n\n* fixed find_hparam calls. Fixed e_score_correction_bias to use bias instead of weight. Removed all ssm_conv bias terms.\n\n* remove DT_B from constants.py. remove one comment line in llama-model.cpp\n\n* new class llm_graph_input_mem_hybrid_k to get around the new MLA change. switch the concat order of ggml_concat calls in kimi-linear.cpp to accommodate MLA changes. Removed support for exp_probs_b.weight\n\n* remove ssm_o_norm_b\n\n* remove ssm_o_norm_b\n\n* changed hparams.kda_head_dim to hparams.n_embd_head_kda. added TODO comment for class llama_graph_mem_hybrid_k\n\n* removed all ggml_cont b4 ggml_reshape_4d\n\n* Whitespace\n\n* replaced all hparams.get with find_hparams\n\n* added new names for n_experts, n_experts_used and score_func in TextModel and removed their code in KimiLinear in convert_hf_to_gguf.py. Removed unnecessary ggml_cont and GGML_ASSERT in kimi-linear.cpp\n\n* use is_mla to switch between different mem_hybrid types\n\n* fixed logical errors in convert_hf_to_gguf.py pointed out by CISC\n\n* removed if else for required parameters kv_lora_rank and qk_rope_head_dim\n\n* add back ggml_cont for Vcur\n\n* minor changes\n\n* removed extra line in llama-vocab.cpp. Added back the comment in llama-graph.cpp\n\n* f16 gguf cannot run without context length\n\n* made a mistake of adding back n_ctx parsing\n\n---------\n\nCo-authored-by: Piotr Wilkin (ilintar) <piotr.wilkin@syndatis.com>"},{"tag":"b7956","published_at":"2026-02-06T14:47:16Z","body":"vulkan: For coopmat2 FA, use fp16 accumulators for the final result (#19376)\n\nThe cpu and cuda backends use fp16 for the VKQ accumulator type, this change\ndoes the same for vulkan. This helps particularly with large head sizes which\nare very register-limited.\n\nI tried this for the coopmat1 path and it slowed down a bit. I didn't try for\nscalar.\n\nI applied the softmax bias that the cuda backend uses to avoid overflow,\nalthough I was not able to reproduce the original bug without it."},{"tag":"b7955","published_at":"2026-02-06T12:37:16Z","body":"vulkan: make FA mask/softcap enables spec constants (#19309)\n\n* vulkan: make FA mask/softcap enables spec constants\n\n* don't specialize for sinks\n\n* bump timeout a little bit"},{"tag":"b7954","published_at":"2026-02-06T11:57:19Z","body":"metal : skip loading all-zero mask (#19337)\n\n* metal : skip loading all-zero mask\n\n* cont : minor"},{"tag":"b7952","published_at":"2026-02-06T09:31:41Z","body":"cuda : cuda graphs now compare all node params (#19383)"}]}
