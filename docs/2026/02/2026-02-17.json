{"executed_at":"2026-02-18T01:52:51Z","data":[{"tag":"b8086","published_at":"2026-02-17T22:42:42Z","body":"opencl: optimize mean and sum_row kernels (#19614)\n\n* opencl: optimize mean and sum_row kernels\n\n* opencl: add comment for max subgroups\n\n* opencl: format\n\n---------\n\nCo-authored-by: Li He <lih@qti.qualcomm.com>"},{"tag":"b8083","published_at":"2026-02-17T13:09:01Z","body":"ggml: ggml-cpu: force-no-lto-for-cpu-feats (#19609)\n\nWhen LTO enabled in build environments it forces all builds to have LTO\nin place. But feature detection logic is fragile, and causing Illegal\ninstruction errors with lto. This disables LTO for the feature\ndetection code to prevent cross-module optimization from inlining\narchitecture-specific instructions into the score function. Without this,\nLTO can cause SIGILL when loading backends on older CPUs (e.g., loading\npower10 backend on power9 crashes before feature check runs)."},{"tag":"b8082","published_at":"2026-02-17T11:57:04Z","body":"cuda : enable CUDA graphs for MMID 1 <= BS <= 4 (#19645)\n\n* cuda : enable CUDA graphs for MMID BS <= 4\n\n* cont : add stream capture check\n\nCo-authored-by: Oliver Simons <osimons@nvidia.com>\n\n* cont : add MMVQ_MMID_MAX_BATCH_SIZE\n\n---------\n\nCo-authored-by: Oliver Simons <osimons@nvidia.com>"},{"tag":"b8079","published_at":"2026-02-17T10:26:55Z","body":"build : link ws2_32 as PUBLIC on Windows (#19666)\n\nSigned-off-by: Adrien Gallouët <adrien@gallouet.fr>"},{"tag":"b8078","published_at":"2026-02-17T10:17:12Z","body":"build : cleanup library linking logic (#19665)\n\nSigned-off-by: Adrien Gallouët <angt@huggingface.co>"},{"tag":"b8077","published_at":"2026-02-17T05:46:19Z","body":"convert : add JoyAI-LLM-Flash (#19651)\n\n* convert_hf_to_gguf: add JoyAI-LLM-Flash tokenizer hash mapping to deepseek-v3\n\n* llama-vocab: create a new pre-tokenizer name for joyai-llm.\n\n* add missing vocab type section\n\n* Update convert_hf_to_gguf_update.py\n\nCo-authored-by: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>\n\n* Update convert_hf_to_gguf.py\n\nCo-authored-by: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>\n\n---------\n\nCo-authored-by: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>"},{"tag":"b8076","published_at":"2026-02-17T04:30:56Z","body":"perplexity: add proper batching (#19661)"},{"tag":"b8075","published_at":"2026-02-17T03:44:37Z","body":"common : inline functions (#18639)"},{"tag":"b8074","published_at":"2026-02-17T02:45:56Z","body":"ggml : make `ggml_is_view` as API (#19539)\n\n* make `ggml_is_view` as API\n\n* introduce `ggml_aux_is_view` as inline version for internal use.\n\n* change `ggml_aux_is_view` to  `ggml_impl_is_view`"},{"tag":"b8073","published_at":"2026-02-17T02:28:54Z","body":"model: Add support for Tiny Aya Models (#19611)\n\n* changes for tiny aya\n\n* changes to hash\n\n* changes to vocab\n\n* fix some tokenizer regex edge cases\n\n* update comment\n\n* add some comments for regex\n\n* Apply suggestion from @ngxson\n\n---------\n\nCo-authored-by: Xuan-Son Nguyen <thichthat@gmail.com>"},{"tag":"b8072","published_at":"2026-02-17T01:02:11Z","body":"build : rework llama_option_depr to handle LLAMA_CURL (#19658)\n\nSigned-off-by: Adrien Gallouët <angt@huggingface.co>"},{"tag":"b8071","published_at":"2026-02-17T00:57:31Z","body":"Adjust workaround for ROCWMMA_FATTN/GFX9 to only newer ROCm veresions (#19591)\n\nAvoids issues with ROCm 6.4.4.\n\nCloses: https://github.com/ggml-org/llama.cpp/issues/19580\nFixes: 6845f7f87 (\"Add a workaround for compilation with ROCWMMA_FATTN and gfx9 (#19461)\")\n\nSigned-off-by: Mario Limonciello (AMD) <superm1@kernel.org>"}]}
