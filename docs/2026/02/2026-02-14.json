{"executed_at":"2026-02-15T01:52:47Z","data":[{"tag":"b8054","published_at":"2026-02-14T22:24:56Z","body":"mtmd : Add Nemotron Nano 12B v2 VL support (#19547)\n\n* nemotron nano v2 vlm support added\n\n* simplified code; addressed reviews\n\n* pre-downsample position embeddings during GGUF conversion for fixed input size"},{"tag":"b8053","published_at":"2026-02-14T22:02:53Z","body":"models : optimize qwen3next graph (#19375)\n\n* models : optimizing qwen3next graph\n\n* cont\n\n* wip\n\n* wip\n\n* wip\n\n* wip\n\n* wip\n\n* wip\n\n* wip\n\n* wip\n\n* wip\n\n* wip\n\n* cont : remove redundant q, g chunking\n\n* minor\n\n* minor\n\n* avoid passing masks around\n\n* avoid concats during chunking\n\n* naming + shapes\n\n* update names and use prefix to disable CUDA graphs"},{"tag":"b8052","published_at":"2026-02-14T21:59:41Z","body":"ggml : fix GGML_DEBUG with OpenMP (#19599)\n\nlast_graph is only available without OpenMP, but\nggml_graph_compute_thread() is called in both cases.\n\nSigned-off-by: Adrien Gallouët <angt@huggingface.co>"},{"tag":"b8051","published_at":"2026-02-14T21:13:43Z","body":"NetBSD build support (#19589)"},{"tag":"b8049","published_at":"2026-02-14T20:42:45Z","body":"llama : update LoRA API. + fix excessive graph reserves (#19280)\n\n* Refactoring to use new llama_put_adapter_loras\n\n* cont : alternative lora API\n\n---------\n\nCo-authored-by: Jake Chavis <jakechavis6@gmail.com>\nCo-authored-by: Georgi Gerganov <ggerganov@gmail.com>"},{"tag":"b8048","published_at":"2026-02-14T20:07:02Z","body":"mmap: Fix Windows handle lifetime (#19598)\n\n* ggml: added cleanups in ggml_quantize_free\nAdd missing cleanup calls for IQ2_S, IQ1_M quantization types and IQ3XS with 512 blocks during quantization cleanup.\n\n* mmap: Fix Windows handle lifetime\nMove hMapping from local variable to member variable so it stays alive for the entire lifetime of the mapping.\nThe file mapping handle must remain valid until UnmapViewOfFile is called.\nFixes cleanup order in destructor.\n\n* Update llama-mmap.cpp\n\n* Update llama-mmap.cpp\n\nRemove trailing whitespace from line 567"},{"tag":"b8047","published_at":"2026-02-14T18:54:42Z","body":"metal : fix ACC op (#19427)"},{"tag":"b8046","published_at":"2026-02-14T18:33:22Z","body":"scripts : use official split.py for cpp-httplib (#19588)\n\n* scripts : use official split.py for cpp-httplib\n\nUsing the official script is safer and ensures the generated code aligns\nwith the library's standards.\n\nSigned-off-by: Adrien Gallouët <angt@huggingface.co>\n\n* Catch generic errors\n\nSigned-off-by: Adrien Gallouët <angt@huggingface.co>\n\n* Allow print()\n\nSigned-off-by: Adrien Gallouët <angt@huggingface.co>\n\n* Ensure robust cleanup\n\nSigned-off-by: Adrien Gallouët <angt@huggingface.co>\n\n---------\n\nSigned-off-by: Adrien Gallouët <angt@huggingface.co>"},{"tag":"b8043","published_at":"2026-02-14T18:13:21Z","body":"vulkan: support L2_NORM with contiguous rows (#19604)"},{"tag":"b8042","published_at":"2026-02-14T18:06:37Z","body":"vulkan: support GGML_OP_SET (#19584)"},{"tag":"b8041","published_at":"2026-02-14T17:03:55Z","body":"vulkan: Add vendor id for Qualcomm drivers (#19569)\n\nThis commit allows Qualcomm native vulkan driver to be used on Windows\ninstead of Mesa Dozen."},{"tag":"b8040","published_at":"2026-02-14T11:33:35Z","body":"hexagon: further optimizations and refactoring for flash attention (#19583)\n\n* ggml-hexagon: fa improvements\n\nggml-hexagon: optimize flash attention calculations with improved variable handling\n\nggml-hexagon: streamline flash attention operations by removing redundant checks for FP32\n\nggml-hexagon: optimize hvx_dot_f16_f16_aa_rx2 by simplifying variable handling for unused elements\n\nggml-hexagon: optimize flash attention by changing slope vector type to F16\n\n* hexfa: fixed test-backend-ops failurs due to leftover element handling\n\n* hexagon: refactor and optimize fa to use local context struct\n\n* ggml-hexagon: optimize flash-attention using hvx_vec_expf\n\nUse HVX for online softmax.\n\n---------\n\nCo-authored-by: chraac <chraac@gmail.com>"},{"tag":"b8038","published_at":"2026-02-14T08:54:49Z","body":"vulkan: restore -inf check in FA shaders (#19582)"},{"tag":"b8037","published_at":"2026-02-14T07:21:45Z","body":"common : update download code (#19573)\n\n* common : remove legacy .json to .etag migration code\n\nSigned-off-by: Adrien Gallouët <angt@huggingface.co>\n\n* common : simplify common_download_file_single_online\n\nThis commit also force a redownload if the file exists\nbut has no .etag file.\n\nSigned-off-by: Adrien Gallouët <angt@huggingface.co>\n\n---------\n\nSigned-off-by: Adrien Gallouët <angt@huggingface.co>"},{"tag":"b8036","published_at":"2026-02-14T06:16:45Z","body":"model: support GLM MoE DSA arch (NOTE: indexer is not yet supported) (#19460)\n\n* model: support GLM MoE DSA arch\n\n* working version\n\n* pyright\n\n* keep indexer tensors\n\n* add indexer gguf params\n\n* loaded now\n\n* Apply suggestions from code review\n\nCo-authored-by: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>\n\n* update\n\n* Update src/llama-model.cpp\n\nCo-authored-by: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>\n\n* minor fix and cleanup\n\n---------\n\nCo-authored-by: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>"},{"tag":"b8035","published_at":"2026-02-14T06:16:14Z","body":"Fix wrong memcpy length for block_interleave == 4 (#19575)"},{"tag":"b8034","published_at":"2026-02-14T06:15:27Z","body":"fix vulkan ggml_acc only works in 3d but not 4d (#19426)\n\n* fix vulkan ggml_acc only works in 3d but not 4d\n\n* removed clamp in test_acc_block\n\n* use the correct stride and its test case\n\n* cuda : fix \"supports op\" condition\n\n* change src0 to src1 in ggml_vk_acc. Update acc.comp with jeffbolznv\\'s suggestion except to keep the boundary check\n\n* version without boundary check\n\n* revert back to boundary check version\n\n---------\n\nCo-authored-by: Georgi Gerganov <ggerganov@gmail.com>"},{"tag":"b8033","published_at":"2026-02-14T05:18:32Z","body":"support --verbose-prompt (#19576)"},{"tag":"b8032","published_at":"2026-02-14T04:52:26Z","body":"CUDA: loop over ne2*ne3 in case it overflows (#19538)\n\n* CUDA: loop over ne2*ne3 in case it overflows\n\n* use fastdiv"},{"tag":"b8030","published_at":"2026-02-14T03:36:31Z","body":"CUDA: Do not mutate cgraph for fused ADDs (#19566)\n\n* Do not mutate cgraph for fused ADDs\n\n1. We should try to minimize in-place changes to the incoming\n   ggml_cgraph where possible (those should happen in graph_optimize)\n2. Modifying in-place leads to an additional, unnecessary graph capture\n   step as we store the properties before modifying the graph in-place\n   in the cuda-backend\n\n* Assert ggml_tensor is trivially copyable\n\n* Update ggml/src/ggml-cuda/ggml-cuda.cu\n\nCo-authored-by: Aman Gupta <amangupta052@gmail.com>\n\n---------\n\nCo-authored-by: Aman Gupta <amangupta052@gmail.com>"},{"tag":"b8028","published_at":"2026-02-14T00:45:50Z","body":"model : Kimi Linear fix conv state update (#19531)\n\n* fix conv state update for llama-server parallel serving\n\n---------\n\nCo-authored-by: Piotr Wilkin (ilintar) <piotr.wilkin@syndatis.com>"}]}
