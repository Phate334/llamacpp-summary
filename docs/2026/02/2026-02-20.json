{"executed_at":"2026-02-21T01:52:12Z","data":[{"tag":"b8118","published_at":"2026-02-20T22:50:24Z","body":"common : merge qwen3-coder and nemotron nano 3 parsers (#19765)\n\n* common : migrate qwen3-coder to PEG parsing variant\n\n* cont : add JSON parameter test"},{"tag":"b8117","published_at":"2026-02-20T13:16:12Z","body":"ggml-cpu: add RVV vec dot kernels for quantization types (#18784)\n\n* ggml-cpu: add rvv vec_dot for iq2_s\n\nCo-authored-by: Rehan Qasim <rehan.qasim@10xengineers.ai>\n\n* ggml-cpu: add rvv vec_dot for iq3_s\n\nCo-authored-by: Rehan Qasim <rehan.qasim@10xengineers.ai>\n\n* ggml-cpu: add rvv vec_dot for tq1_0, tq2_0\n\nCo-authored-by: Rehan Qasim <rehan.qasim@10xengineers.ai>\n\nggml-cpu: add rvv vec_dot for tq1_0, tq2_0\n\n* ggml-cpu: add rvv vec_dot for iq1_s, iq1_m\n\nCo-authored-by: Rehan Qasim <rehan.qasim@10xengineers.ai>\n\n* ggml-cpu: add vlen switch for rvv vec_dot\n\n---------\n\nCo-authored-by: Rehan Qasim <rehan.qasim@10xengineers.ai>"},{"tag":"b8116","published_at":"2026-02-20T10:16:56Z","body":"quantize : add --dry-run option (#19526)\n\n* clean slate for branch\n\n* use 6 characters for tensor dims\n\n* add --dry-run to llama-quantize\n\n* use 6 characters for tensor dims (cont.)\n\n* no need to re-calculate ggml_nbytes for tensor\n\n* fix indent\n\n* show model and quant BPW when quant completes\n\n* add example to --help\n\n* new function `tensor_requires_imatrix`, add courtesy warning about imatrix\n\n* missing __func__, move imatrix flag set\n\n* logic error\n\n* fixup tensor_requires_imatrix\n\n* add missing `GGML_TYPE`s\n\n* simplify and rename `tensor_type_requires_imatrix`\n\n* simplify for style\n\n* add back Q2_K edge case for imatrix\n\n* guard ftype imatrix warning\n\n* comment ref #12557\n\n* remove per @compilade\n\n* remove unused `params` parameter\n\n* move `bool dry_run` per GG\n\n* move `bool dry_run` per GG\n\n* Update src/llama-quant.cpp\n\nCo-authored-by: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>\n\n* Update src/llama-quant.cpp\n\nCo-authored-by: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>\n\n* Update src/llama-quant.cpp\n\nCo-authored-by: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>\n\n---------\n\nCo-authored-by: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>"},{"tag":"b8115","published_at":"2026-02-20T05:05:10Z","body":"test: mul_mat tests with huge batch size (#19519)"},{"tag":"b8113","published_at":"2026-02-20T05:03:46Z","body":"common : fix Step-3.5-Flash format detection and thinking support (#19635)\n\n* common : fix Step-3.5-Flash format detection and thinking support\n\nStep-3.5-Flash uses the same XML-style tool call format as Qwen3-Coder\n(<tool_call><function=...><parameter=...>) but its Jinja template lacks\nthe bare <function> and plural <parameters> markers that the detection\nlogic previously required. This caused it to fall through to Hermes 2\nPro, which doesn't call func_args_not_string(), so arguments stayed as\nJSON strings and templates using arguments|items crashed.\n\nAdditionally, the Qwen3-Coder-XML format handler had no thinking support.\nModels like Step-3.5-Flash that unconditionally emit <think> in their\ngeneration prompt need the same thinking_forced_open handling that\nNemotron v3 and Hermes 2 Pro already have, otherwise reasoning_content\nis never separated from content in API responses.\n\nChanges:\n- Relax Qwen3-Coder XML detection to only require the 3 shared markers\n- Tighten Nemotron v3 branch to also require bare <function> and plural\n  <parameters>, preventing Step-3.5-Flash from being misrouted via <think>\n- Add thinking_forced_open support to Qwen3-Coder-XML init function\n- Add <think>/</think> to preserved tokens\n- Fix build_grammar_xml_tool_call to handle thinking_forced_open in the\n  grammar root rule, allowing </think> before tool calls\n- Add Step-3.5-Flash chat template and format detection test\n\nBuilds on: https://github.com/ggml-org/llama.cpp/pull/19283\n\n* chat : route Step-3.5-Flash to Nemotron v3 PEG parser, add tests\n\nStep-3.5-Flash uses the same XML tool call format as Qwen3-Coder and\nNemotron 3 Nano (<tool_call>/<function=...>/<parameter=...>) but with\nunconditional <think> output. Route it to the Nemotron v3 PEG parser\nfor streaming and schema-aware parameter parsing.\n\nDetection: templates with <think> + XML tool tags use Nemotron v3 PEG\nparser; templates without <think> (Qwen3-Coder) use GBNF grammar.\n\nTests cover: basic messages, tool calls with/without thinking content,\nparallel tool calls, code string parameters, optional </parameter>\nclosing tags, and JSON schema response format.\n\n* chat : remove dead thinking code from qwen3_coder_xml\n\nRemove thinking handling code that became unreachable after routing\nStep-3.5-Flash to the Nemotron v3 PEG parser. Qwen3-Coder has no\n<think> in its template, so the thinking_forced_open logic, preserved\ntokens, and grammar prefix were dead paths."},{"tag":"b8112","published_at":"2026-02-20T04:54:11Z","body":"common : fix gpt-oss Jinja error when assistant message has both content and thinking with tool calls (#19704)"},{"tag":"b8111","published_at":"2026-02-20T04:29:09Z","body":"ggml-webgpu: Add unary op (SQR, SQRT, SIN, COS) support. (#19700)\n\n* ggml-webgpu: Add unary op (SQR, SQRT, SIN, COS) support.\n\n* Fix to cast the src value to f32 before sin/cos computing."},{"tag":"b8110","published_at":"2026-02-20T04:04:36Z","body":"model: Add PaddleOCR-VL model support (#18825)\n\n* support PaddleOCR-VL\n\n* clip: update PaddleOCR model loader parameters to prevent OOM during warmup\n\n* [update] add paddleocr vl text model instead of ernie4.5\n\n* [update] restore change of minicpmv\n\n* [update] format\n\n* [update] format\n\n* [update] positions and patch merge permute\n\n* [update] mtmd_decode_use_mrope for paddleocr\n\n* [update] image min/max pixels\n\n* [update] remove set_limit_image_tokens\n\n* upate: preprocess without padding\n\n* clean up\n\n* Update convert_hf_to_gguf.py\n\nCo-authored-by: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>\n\n* Update convert_hf_to_gguf.py\n\nCo-authored-by: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>\n\n---------\n\nCo-authored-by: Xuan Son Nguyen <son@huggingface.co>\nCo-authored-by: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>"},{"tag":"b8109","published_at":"2026-02-20T03:36:06Z","body":"vulkan: fix MMQ shader push constants and multi-dispatch (#19732)"},{"tag":"b8108","published_at":"2026-02-20T00:50:08Z","body":"models : fix qwen3.5 beta/gate shapes (#19730)\n\n* models : fix qwen3.5 beta/gate shapes\n\n* cont : avoid extra reshapes"},{"tag":"b8107","published_at":"2026-02-20T00:44:11Z","body":"mtmd: build_attn modified, flash_attn on/off via ctx_params (#19729)"},{"tag":"b8106","published_at":"2026-02-20T00:06:52Z","body":"model : add JAIS-2 architecture support (#19488)\n\n* model: add JAIS-2 architecture support\n\nAdd support for the JAIS-2 family of Arabic-English bilingual models\nfrom Inception AI (https://huggingface.co/inceptionai/Jais-2-8B-Chat).\n\nArchitecture characteristics:\n- LayerNorm (not RMSNorm) with biases\n- ReLU² (ReLU squared) activation function\n- Separate Q/K/V projections with biases\n- Simple MLP without gate projection (up -> act -> down)\n- RoPE positional embeddings\n- GPT-2 BPE tokenizer\n\nSupported model sizes:\n- Jais-2-8B (32 layers, 26 heads, 3328 hidden)\n- Jais-2-70B (68 layers, 56 heads, 7168 hidden)\n\nTested with quantizations: BF16, Q8_0, Q6_K, Q5_K_M, Q5_0, Q4_K_M, Q4_0, Q3_K_M, Q2_K\n\nNote: JAIS-2 requires F32 precision accumulators for numerical stability\nand uses standard attention (not flash attention) on CUDA backends.\n\n* fix: run convert_hf_to_gguf_update.py for jais-2 tokenizer hash\n\n* fix: use NEOX RoPE type for JAIS2\n\n* fix: remove Q/K permutation (NEOX RoPE doesn't need it)\n\n* fix: enable flash attention for JAIS2 (fixed by #19115)\n\n* fix: add dedicated JAIS2 pre-tokenizer type and control vector support\n\n- Add LLAMA_VOCAB_PRE_TYPE_JAIS2 with cascading whitespace regex\n- Include original regex from tokenizer.json as comment\n- Add build_cvec call for control vector support\n\n* no longer necessary to override set_vocab\n\n---------\n\nCo-authored-by: Sigbjørn Skjæret <sigbjorn.skjaeret@scala.com>"}]}
